#!/usr/bin/env python3
"""
LeRobot Dataset to RoboDM Training Pipeline

This script demonstrates the complete pipeline:
1. Load real data from LeRobot datasets (pusht, xarm, aloha, etc.)
2. Convert the data to RoboDM format for parallel processing
3. Create a bridge back to LeRobot format for training
4. Train a policy using LeRobot's training pipeline

Usage:
    python robodm_lerobot_training.py --dataset lerobot/pusht --num_episodes 50
"""

import os
import tempfile
import time
import argparse
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

import numpy as np
import torch
import torch.utils.data as torch_data

# RoboDM imports
import robodm
from robodm.dataset import VLADataset, DatasetConfig
from robodm.trajectory import Trajectory

# LeRobot imports (if available)
try:
    from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata
    from lerobot.configs.types import FeatureType
    from lerobot.policies.diffusion.configuration_diffusion import DiffusionConfig
    from lerobot.policies.diffusion.modeling_diffusion import DiffusionPolicy
    # Set backend to pyav for video processing
    import lerobot.datasets.video_utils as video_utils
    if hasattr(video_utils, 'set_video_backend'):
        video_utils.set_video_backend('pyav')
    LEROBOT_AVAILABLE = True
except ImportError:
    print("LeRobot not available. Will only demonstrate RoboDM data generation and conversion.")
    LEROBOT_AVAILABLE = False


class SimpleRoboDMToLeRobotBridge:
    """Minimal bridge to convert RoboDM data to LeRobot format."""
    
    def __init__(self, robodm_dataset: VLADataset):
        self.robodm_dataset = robodm_dataset
        
        # Load trajectories if not already loaded
        if not robodm_dataset._is_loaded:
            print("Loading trajectories for bridge...")
            self.robodm_dataset = robodm_dataset.load_trajectories()
        
        # Create PyTorch dataset
        print("Creating PyTorch dataset from RoboDM data...")
        self.torch_dataset = self._create_torch_dataset()
    
    def _create_torch_dataset(self) -> torch_data.Dataset:
        """Convert RoboDM dataset to PyTorch dataset."""
        # Get all trajectories - properly materialize the dataset
        ray_dataset = self.robodm_dataset.get_ray_dataset()
        # if not ray_dataset._is_materialized:
        #     ray_dataset = ray_dataset.materialize()
        trajectories = list(ray_dataset.iter_rows())
        
        print(f"Converting {len(trajectories)} trajectories...")
        
        # Convert each trajectory to timesteps
        all_timesteps = []
        for episode_idx, traj in enumerate(trajectories):
            try:
                timesteps = self._convert_trajectory(traj, episode_idx)
                all_timesteps.extend(timesteps)
                if (episode_idx + 1) % 10 == 0:
                    print(f"  Processed {episode_idx + 1}/{len(trajectories)} trajectories")
            except Exception as e:
                print(f"  Warning: Failed to convert trajectory {episode_idx}: {e}")
                continue
        
        print(f"Created dataset with {len(all_timesteps)} timesteps")
        return SimplePyTorchDataset(all_timesteps)
    
    def _convert_trajectory(self, trajectory: Dict[str, Any], episode_idx: int) -> List[Dict[str, torch.Tensor]]:
        """Convert single trajectory to list of timesteps."""
        # Find trajectory length from available data
        traj_len = 0
        image_keys = [k for k in trajectory.keys() if 'observation/image' in k or 'observation/images' in k]
        state_keys = [k for k in trajectory.keys() if 'observation/state' in k]
        action_keys = [k for k in trajectory.keys() if 'action' in k]
        
        # Determine trajectory length from the first available data source
        if image_keys and len(trajectory[image_keys[0]]) > 0:
            traj_len = len(trajectory[image_keys[0]])
        elif action_keys and len(trajectory[action_keys[0]]) > 0:
            traj_len = len(trajectory[action_keys[0]])
        elif state_keys and len(trajectory[state_keys[0]]) > 0:
            traj_len = len(trajectory[state_keys[0]])
        else:
            return []  # No valid data found
        
        timesteps = []
        for frame_idx in range(traj_len):
            # Create timestep data in LeRobot format
            timestep = {
                'timestamp': torch.tensor([frame_idx * 0.1], dtype=torch.float32),  # 10 FPS
                'frame_index': torch.tensor([frame_idx], dtype=torch.int64),
                'episode_index': torch.tensor([episode_idx], dtype=torch.int64),
                'index': torch.tensor([len(timesteps)], dtype=torch.int64),
                'task_index': torch.tensor([0], dtype=torch.int64),
            }
            
            # Add image observations
            if image_keys:
                primary_image_key = image_keys[0]  # Use first available image
                if frame_idx < len(trajectory[primary_image_key]):
                    image_data = trajectory[primary_image_key][frame_idx]
                    if isinstance(image_data, np.ndarray):
                        # Make a copy to ensure the array is writable
                        image_data = image_data.copy()
                        # Convert to tensor, ensure it's in CHW format
                        if len(image_data.shape) == 3 and image_data.shape[2] == 3:  # HWC format
                            image_tensor = torch.from_numpy(image_data).permute(2, 0, 1).float() / 255.0
                        else:  # Already in CHW format
                            image_tensor = torch.from_numpy(image_data).float() / 255.0
                        timestep['observation.image'] = image_tensor
            
            # Add state observations
            if state_keys:
                state_data = trajectory[state_keys[0]][frame_idx] if frame_idx < len(trajectory[state_keys[0]]) else np.array([])
                if isinstance(state_data, np.ndarray) and len(state_data) > 0:
                    state_data = state_data.copy()  # Make writable
                    timestep['observation.state'] = torch.from_numpy(state_data).float()
            
            # Add actions
            if action_keys:
                action_data = trajectory[action_keys[0]][frame_idx] if frame_idx < len(trajectory[action_keys[0]]) else np.array([])
                if isinstance(action_data, np.ndarray) and len(action_data) > 0:
                    action_data = action_data.copy()  # Make writable
                    timestep['action'] = torch.from_numpy(action_data).float()
            
            timesteps.append(timestep)
        
        return timesteps
    
    def get_torch_dataset(self) -> torch_data.Dataset:
        """Get PyTorch dataset."""
        return self.torch_dataset
    
    def get_features_info(self) -> Dict[str, Dict[str, Any]]:
        """Get feature information for LeRobot policy configuration."""
        sample = self.torch_dataset[0]
        
        return {
            'observation.image': {
                'dtype': 'image',
                'shape': list(sample['observation.image'].shape),  # [C, H, W]
                'names': None
            },
            'observation.state': {
                'dtype': 'float32', 
                'shape': list(sample['observation.state'].shape),
                'names': None
            },
            'action': {
                'dtype': 'float32',
                'shape': list(sample['action'].shape), 
                'names': None
            }
        }
    
    def get_dataset_stats(self) -> Dict[str, Dict[str, torch.Tensor]]:
        """Calculate dataset statistics for normalization."""
        print("Calculating dataset statistics...")
        
        # Collect all data
        all_images = []
        all_states = []
        all_actions = []
        
        for item in self.torch_dataset:
            all_images.append(item['observation.image'])
            all_states.append(item['observation.state'])
            all_actions.append(item['action'])
        
        # Stack and calculate stats
        images = torch.stack(all_images)
        states = torch.stack(all_states) 
        actions = torch.stack(all_actions)
        
        stats = {
            'observation.image': {
                'mean': images.mean(dim=0),
                'std': images.std(dim=0),
                'min': images.min(dim=0)[0],
                'max': images.max(dim=0)[0]
            },
            'observation.state': {
                'mean': states.mean(dim=0),
                'std': states.std(dim=0),
                'min': states.min(dim=0)[0],
                'max': states.max(dim=0)[0]
            },
            'action': {
                'mean': actions.mean(dim=0),
                'std': actions.std(dim=0),
                'min': actions.min(dim=0)[0],
                'max': actions.max(dim=0)[0]
            }
        }
        
        return stats


class SimplePyTorchDataset(torch_data.Dataset):
    """Simple PyTorch dataset wrapper."""
    
    def __init__(self, data: List[Dict[str, torch.Tensor]]):
        self.data = data
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        return self.data[idx]


def load_lerobot_dataset_to_robodm(dataset_name: str, num_episodes: Optional[int] = None, save_dir: str = None) -> str:
    """Load LeRobot dataset and convert to RoboDM format."""
    
    if not LEROBOT_AVAILABLE:
        raise ImportError("LeRobot is not available. Please install lerobot package.")
    
    if save_dir is None:
        save_dir = tempfile.mkdtemp(prefix="robodm_lerobot_")
    
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"Loading LeRobot dataset: {dataset_name}")
    
    # Get dataset metadata first
    try:
        meta = LeRobotDatasetMetadata(dataset_name)
        print(f"Dataset info: {meta.total_episodes} episodes, {meta.total_frames} frames")
    except Exception as e:
        print(f"Could not load metadata: {e}. Proceeding without metadata.")
        meta = None
        # For some datasets, we might want to continue with a warning
        print(f"Warning: Dataset {dataset_name} may not be fully compatible. Continuing anyway...")
    
    # Determine episodes to load
    if num_episodes is not None and meta is not None:
        episodes_to_load = list(range(min(num_episodes, meta.total_episodes)))
    else:
        episodes_to_load = None
    
    # Load LeRobot dataset with pyav backend
    print(f"Loading dataset with episodes: {episodes_to_load if episodes_to_load else 'all'}")
    # Use pyav backend for video processing if available
    try:
        lerobot_dataset = LeRobotDataset(dataset_name, episodes=episodes_to_load, video_backend='pyav')
    except TypeError:
        # Fallback if video_backend parameter is not supported
        lerobot_dataset = LeRobotDataset(dataset_name, episodes=episodes_to_load)
    
    print(f"Dataset loaded with {len(lerobot_dataset)} samples")
    
    # Convert to RoboDM format by episodes
    episodes_data = {}
    for i, sample in enumerate(lerobot_dataset):
        episode_idx = sample['episode_index'].item()
        frame_idx = sample['frame_index'].item()
        
        if episode_idx not in episodes_data:
            episodes_data[episode_idx] = []
        
        episodes_data[episode_idx].append((frame_idx, sample))
    
    # Sort each episode by frame index
    for episode_idx in episodes_data:
        episodes_data[episode_idx].sort(key=lambda x: x[0])
    
    print(f"Converting {len(episodes_data)} episodes to RoboDM format...")
    print(f"Saving to: {save_dir}")
    
    # Convert each episode to RoboDM trajectory
    for episode_idx, frames in episodes_data.items():
        trajectory_path = os.path.join(save_dir, f"episode_{episode_idx:03d}.vla")
        traj = Trajectory(path=trajectory_path, mode="w")
        
        try:
            for frame_idx, sample in frames:
                # Convert timestamp (assuming 10 FPS by default)
                timestamp = frame_idx * 100  # 100ms intervals = 10 FPS
                
                # Add observations
                if 'observation.image' in sample:
                    # Convert from CHW to HWC format
                    image = sample['observation.image'].permute(1, 2, 0).numpy()
                    # Convert from [0,1] to [0,255] if needed
                    if image.max() <= 1.0:
                        image = (image * 255).astype(np.uint8)
                    traj.add("observation/image", image, timestamp=timestamp, time_unit="ms")
                
                # Handle multiple camera observations
                for key in sample.keys():
                    if key.startswith('observation.images.'):
                        camera_name = key.split('.')[-1]
                        image = sample[key].permute(1, 2, 0).numpy()
                        if image.max() <= 1.0:
                            image = (image * 255).astype(np.uint8)
                        traj.add(f"observation/images/{camera_name}", image, timestamp=timestamp, time_unit="ms")
                    elif key.startswith('observation.image') and key != 'observation.image':
                        # Handle other image observations like observation.image_front, etc.
                        camera_name = key.split('.')[-1] if '.' in key else key.replace('observation.', '')
                        image = sample[key].permute(1, 2, 0).numpy()
                        if image.max() <= 1.0:
                            image = (image * 255).astype(np.uint8)
                        traj.add(f"observation/images/{camera_name}", image, timestamp=timestamp, time_unit="ms")
                
                if 'observation.state' in sample:
                    state = sample['observation.state'].numpy().astype(np.float32)
                    traj.add("observation/state", state, timestamp=timestamp, time_unit="ms")
                
                # Add actions
                if 'action' in sample:
                    action = sample['action'].numpy().astype(np.float32)
                    traj.add("action", action, timestamp=timestamp, time_unit="ms")
                
                # Add reward and done signals if available
                if 'next.reward' in sample:
                    reward = sample['next.reward'].numpy().astype(np.float32)
                    traj.add("reward", reward, timestamp=timestamp, time_unit="ms")
                
                if 'next.done' in sample:
                    done = sample['next.done'].numpy().astype(np.bool_)
                    traj.add("done", done, timestamp=timestamp, time_unit="ms")
        
        finally:
            traj.close()
    
    print(f"‚úÖ Converted {len(episodes_data)} episodes to RoboDM format successfully!")
    return save_dir


def load_robodm_dataset(data_dir: str) -> VLADataset:
    """Load RoboDM dataset from directory and properly materialize it."""
    print(f"Loading RoboDM dataset from: {data_dir}")
    
    config = DatasetConfig(
        batch_size=4,
        shuffle=False,
        num_parallel_reads=2,
        use_metadata=False,  # Skip metadata for simplicity
    )
    
    dataset = VLADataset(
        path=f"{data_dir}/*.vla",  # Load all .vla files
        return_type="numpy",
        config=config
    )
    
    print(f"Found {dataset.count()} trajectory files")
    
    # Load trajectories in parallel using the proper RoboDM interface
    print("Loading trajectories in parallel...")
    loaded_dataset = dataset.load_trajectories()
    
    print(f"‚úÖ Loaded dataset with {loaded_dataset.count()} trajectories")
    return loaded_dataset


def demo_lerobot_training(bridge: SimpleRoboDMToLeRobotBridge):
    """Demonstrate training with LeRobot (if available)."""
    if not LEROBOT_AVAILABLE:
        print("‚ùå LeRobot not available, skipping training demo")
        return
    
    print("üöÄ Starting LeRobot training demo...")
    
    # Get dataset and features
    torch_dataset = bridge.get_torch_dataset()
    features_info = bridge.get_features_info()
    dataset_stats = bridge.get_dataset_stats()
    
    print(f"Dataset size: {len(torch_dataset)}")
    print(f"Features: {list(features_info.keys())}")
    
    # Create feature configurations for policy
    from lerobot.configs.types import PolicyFeature
    
    input_features = {}
    output_features = {}
    
    for key, info in features_info.items():
        feature = PolicyFeature(
            type=FeatureType.STATE if info['dtype'] != 'image' else FeatureType.VISUAL,
            shape=info['shape']
        )
        
        if 'action' in key:
            output_features[key] = feature
        else:
            input_features[key] = feature
    
    print(f"Input features: {list(input_features.keys())}")
    print(f"Output features: {list(output_features.keys())}")
    
    # For this demo, we'll just show that the data is ready for training
    # rather than actually instantiate and train the policy
    print("‚úÖ Data successfully converted and ready for LeRobot training!")
    print("\nData format verification:")
    print(f"- Total samples: {len(torch_dataset)}")
    print(f"- Input features: {list(input_features.keys())}")
    print(f"- Output features: {list(output_features.keys())}")
    print(f"- Dataset statistics calculated: {list(dataset_stats.keys())}")
    
    # Show data loader functionality
    dataloader = torch_data.DataLoader(
        torch_dataset,
        batch_size=4,
        shuffle=True,
        drop_last=True
    )
    
    print("\nData loader test:")
    batch = next(iter(dataloader))
    print(f"- Batch size: {len(batch['episode_index'])}")
    print(f"- Image batch shape: {batch['observation.image'].shape}")
    print(f"- State batch shape: {batch['observation.state'].shape}")
    print(f"- Action batch shape: {batch['action'].shape}")
    
    print("\nüîß To use this data with LeRobot's full training pipeline:")
    print("1. Use the converted RoboDM trajectories for parallel processing")
    print("2. Apply filters using the Agent system as shown in droid_vlm_demo.py")
    print("3. Create proper LeRobot configs and run training with lerobot train")
    print("4. The bridge we created can be used to interface with any PyTorch training loop")
    
    print("\n‚úÖ Training pipeline demo completed successfully!")


def main():
    """Main function demonstrating the complete pipeline."""
    print("ü§ñ LeRobot -> RoboDM -> LeRobot Training Pipeline")
    print("=" * 60)
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="LeRobot to RoboDM training pipeline")
    parser.add_argument("--dataset", type=str, default="lerobot/pusht", 
                       help="LeRobot dataset name (e.g., lerobot/pusht, lerobot/xarm_lift_medium)")
    parser.add_argument("--num_episodes", type=int, default=10, 
                       help="Number of episodes to load (None for all). Default reduced to 10 for faster testing.")
    parser.add_argument("--save_dir", type=str, default=None, 
                       help="Directory to save RoboDM trajectories")
    parser.add_argument("--skip_training", action="store_true", 
                       help="Skip the training demo and only do data conversion")
    args = parser.parse_args()
    
    print(f"Configuration:")
    print(f"  Dataset: {args.dataset}")
    print(f"  Episodes: {args.num_episodes}")
    print(f"  Save dir: {args.save_dir or 'temporary'}")
    print(f"  Skip training: {args.skip_training}")
    
    # Step 1: Load LeRobot dataset and convert to RoboDM
    print("\nüìä Step 1: Loading LeRobot dataset and converting to RoboDM...")
    data_dir = load_lerobot_dataset_to_robodm(
        dataset_name=args.dataset,
        num_episodes=args.num_episodes,
        save_dir=args.save_dir
    )
    
    # Step 2: Load RoboDM dataset
    print("\nüìÇ Step 2: Loading RoboDM dataset...")
    robodm_dataset = load_robodm_dataset(data_dir)
    
    # Step 3: Create bridge to LeRobot format
    print("\nüåâ Step 3: Creating bridge to LeRobot format...")
    bridge = SimpleRoboDMToLeRobotBridge(robodm_dataset)
    
    # Step 4: Demo conversion
    print("\nüîÑ Step 4: Testing data conversion...")
    torch_dataset = bridge.get_torch_dataset()
    print(f"PyTorch dataset created with {len(torch_dataset)} samples")
    
    # Show sample data
    sample = torch_dataset[0]
    print("Sample data shapes:")
    for key, value in sample.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape} ({value.dtype})")
    
    # Step 5: Demo LeRobot training (if available)
    if not args.skip_training:
        print("\nüöÄ Step 5: LeRobot training demo...")
        demo_lerobot_training(bridge)
    else:
        print("\n‚è≠Ô∏è  Step 5: Skipping training demo as requested")
    
    print(f"\n‚úÖ Demo completed! Data saved in: {data_dir}")
    print("You can now use this data with LeRobot's training pipeline.")


if __name__ == "__main__":
    main()